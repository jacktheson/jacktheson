{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnfinal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacktheson/jacktheson/blob/main/nnfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPhzNQLwVg_C"
      },
      "source": [
        "**Recipe Robot:**\n",
        "Sophia, Jackson, Juancy and Mia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZCBFIqTK_Eg"
      },
      "source": [
        "Generally: Use a GAN for ingredients and a RNN (LSTM) for the instructions\n",
        "\n",
        "Steps:\n",
        "  1. encode this, e.g. a vector of 0's, with 1's corresponding to the words that appear in the recipe\n",
        "  2. make a GAN that generates new believable recipe lists in the same way you can generate believable faces or handwritten digits\n",
        "  3. Use an RNN(LSTM) to create the recipe instructions using the generated ingredients as a seed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlmC4_6m6rBV"
      },
      "source": [
        "import csv\n",
        "import re \n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py_Hc_0f6xI7"
      },
      "source": [
        "`CREATING VECTORS`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "4CMkecLG600p",
        "outputId": "4a667caf-d150-4708-8787-a5e81f458348"
      },
      "source": [
        "import csv\n",
        "import re \n",
        "\n",
        "datafile = open('RecipeNLG_dataset.csv', 'r')\n",
        "datareader = csv.reader(datafile, delimiter=',')\n",
        "originalData = []\n",
        "for row in datareader:\n",
        "    originalData.append(row)    \n",
        "#print (originalData[1])\n",
        "\n",
        "for item in originalData[1]:\n",
        "    print (item + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6105d3d2e9ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdatafile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RecipeNLG_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdatareader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moriginalData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RecipeNLG_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE7-k72m991F"
      },
      "source": [
        "titleIngred = []\n",
        "\n",
        "for row in originalData:\n",
        "        titleIngred.append(row[1] + \" ; \" + row[6])\n",
        "        \n",
        "titleIngred.pop(0);\n",
        "        \n",
        "print(titleIngred[0:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf6hncNg-AOp"
      },
      "source": [
        "ingredListMajor = []\n",
        "\n",
        "for ingred in titleIngred:\n",
        "    ingredString = ingred.split(\" ; \")[1]\n",
        "    regex = re.compile('[^a-zA-Z| |,]')\n",
        "    temp = regex.sub('', ingredString).split(\", \")\n",
        "    for item in temp:\n",
        "        ingredListMajor.append(item.lower())\n",
        "    \n",
        "print(ingredListMajor[0:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7v8fRGZ-Dmu"
      },
      "source": [
        "ingredients = list(set(ingredListMajor))\n",
        "ingredients.pop(0)\n",
        "print(ingredients)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPc1omGi-Fof"
      },
      "source": [
        "ingredientVectors = [[]]\n",
        "\n",
        "ingredientVectors.append(ingredients)\n",
        "ingredientVectors.pop(0)\n",
        "\n",
        "for index in range(len(titleIngred)):\n",
        "    counter = []\n",
        "    for ingred in ingredients:\n",
        "        if titleIngred[index].count(ingred)>0:\n",
        "            counter.append(1)\n",
        "        else:\n",
        "            counter.append(0)\n",
        "    ingredientVectors.append(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V89zLTW-Ihp"
      },
      "source": [
        "print(ingredientVectors[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76dYt8RZ-LBD"
      },
      "source": [
        "ingredientVectors2 = np.array([], ndmin=2)\n",
        "\n",
        "np.append(ingredientVectors2, ingredients)\n",
        "#ingredientVectors.pop(0)\n",
        "\n",
        "for index in range(len(titleIngred)):\n",
        "    counter = []\n",
        "    for ingred in ingredients:\n",
        "        if titleIngred[index].count(ingred)>0:\n",
        "            counter.append(1)\n",
        "        else:\n",
        "            counter.append(0)\n",
        "    np.append(ingredientVectors2, counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF7jY3Fy7vy8"
      },
      "source": [
        "# **gan**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4piDGaj88OI4"
      },
      "source": [
        "import csv\n",
        "import re \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LeakyReLU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "vQ1W8eEF8RmJ",
        "outputId": "87eb8fc5-4653-490d-a702-3a852b9cf362"
      },
      "source": [
        "# importing original csv file \n",
        "datafile = open('RecipeNLG_dataset.csv', 'r')\n",
        "datareader = csv.reader(datafile, delimiter=',')\n",
        "originalData = []\n",
        "for row in datareader:\n",
        "    originalData.append(row)    \n",
        "\n",
        "for item in originalData[1]:\n",
        "    print (item + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3b0bc537e5f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# importing original csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatafile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RecipeNLG_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdatareader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moriginalData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatareader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RecipeNLG_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGLhQcAs8VV5"
      },
      "source": [
        "# formatting first 1,000,000 data values to just be recipe title + ingredient list, separated by ';'\n",
        "\n",
        "titleIngred = []\n",
        "\n",
        "for row in originalData:\n",
        "#for row in originalData[:1000000]:\n",
        "    titleIngred.append(row[1] + \" ; \" + row[6])\n",
        "        \n",
        "titleIngred.pop(0);\n",
        "        \n",
        "print(titleIngred[0:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS92vNgO8YRl"
      },
      "source": [
        "# list of all ingredients, repeats included\n",
        "\n",
        "ingredListMajor = []\n",
        "\n",
        "for ingred in titleIngred:\n",
        "    ingredString = ingred.split(\" ; \")[1]\n",
        "    regex = re.compile('[^a-zA-Z| |,-./\\s]')\n",
        "    temp = regex.sub('', ingredString).split(\", \")\n",
        "    for item in temp:\n",
        "        if item != \"egg\" and len(item)>2:\n",
        "            ingredListMajor.append(item.lower())\n",
        "\n",
        "print(ingredListMajor[0:100])\n",
        "print(\"LENGTH: \", len(ingredListMajor))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1plcMK28bdK"
      },
      "source": [
        "# list of 1,000 most common ingredients\n",
        "\n",
        "ingredients = []\n",
        "\n",
        "for item in Counter(ingredListMajor).most_common(1000):\n",
        "    ingredients.append(item[0])\n",
        "    \n",
        "print(ingredients)\n",
        "print(\"LENGTH: \", len(ingredients))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "appGXK4a8d9T"
      },
      "source": [
        "# double list of zeros + ones, 1 representing ingredient is present in recipe\n",
        "# if recipe has less than 5 of the most common ingredients, it is tossed\n",
        "\n",
        "ingredientVectors = [[]]\n",
        "\n",
        "ingredientVectors.append(ingredients)\n",
        "\n",
        "for index in range(len(titleIngred)):\n",
        "    counter = []\n",
        "    for ingred in ingredients:\n",
        "        if titleIngred[index].count(ingred)>0:\n",
        "            counter.append(1)\n",
        "        else:\n",
        "            counter.append(0)\n",
        "    if counter.count(1)>=5:\n",
        "        ingredientVectors.append(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ixkTOnP8jTZ"
      },
      "source": [
        "# numpy array of ingredient vectors created above \n",
        "\n",
        "npIngredients = np.array(ingredientVectors[2:])\n",
        "print(npIngredients)\n",
        "print(npIngredients.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsS9LDMx8lUg"
      },
      "source": [
        "# create list of ingredients + directions properly formatted\n",
        "# later will turn this into a string and run through LSTM\n",
        "\n",
        "ingredSteps = []\n",
        "\n",
        "for line in originalData:\n",
        "    new = line[3].split('\", ')\n",
        "    regex = re.compile('[^a-zA-Z| |, | 0-9]')\n",
        "\n",
        "    steps = []\n",
        "\n",
        "    for part in new:\n",
        "        steps.append(regex.sub('', part))\n",
        "    \n",
        "    counter = 1\n",
        "    stepsFinal = \"\"\n",
        "\n",
        "    for part in steps:\n",
        "        temp = str(counter) + \". \" + part\n",
        "        stepsFinal = stepsFinal + temp + \"\\n\"\n",
        "        counter=counter+1\n",
        "    \n",
        "    line2 = line[6]\n",
        "    regex = re.compile('[^a-zA-Z| |,]')\n",
        "    temp = regex.sub('', line2).split(\", \")\n",
        "\n",
        "    tempIngred = \"\"\n",
        "\n",
        "    for index in range(len(temp)):\n",
        "        if index==0:\n",
        "            tempIngred = temp[0]\n",
        "        else: tempIngred += \", \" + temp[index]\n",
        "\n",
        "    final = (\"INGREDIENTS: \" + tempIngred + \"\\n\\n\" + stepsFinal).rstrip(stepsFinal[-1])\n",
        "    ingredSteps.append(final)\n",
        "\n",
        "for line in ingredSteps[1:10]:\n",
        "    print(line)\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J_jhmZ48s92"
      },
      "source": [
        "def build_generator(input_shape, latent_dim):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(100, input_dim=latent_dim, activation = 'relu')) \n",
        "    generator.add(Dense(200, activation = 'relu'))\n",
        "    generator.add(Dense(input_shape, activation = 'relu'))\n",
        "    return generator\n",
        "def build_discriminator(input_shape):\n",
        "    discriminator = Sequential()\n",
        "    #discriminator.add(Flatten(input_shape=input_shape))\n",
        "    discriminator.add(Dense(100, input_dim=input_shape))\n",
        "    discriminator.add(LeakyReLU(alpha=0.01))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    return discriminator\n",
        "def build_gan(generator, discriminator):\n",
        "    model = Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I79kUGwX82XW"
      },
      "source": [
        "input_shape = (len(npIngredients[0]))\n",
        "print(input_shape)\n",
        "latent_dim = 50\n",
        "\n",
        "discriminator = build_discriminator(input_shape)\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "                      optimizer=Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "discriminator.trainable = False\n",
        "generator = build_generator(input_shape, latent_dim)\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfPOXSag84fb"
      },
      "source": [
        "storeloss = []\n",
        "accuracies = []\n",
        "iteration_checkpoints = []\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "    \n",
        "    # Attempt to load data \n",
        "    X_train = npIngredients\n",
        "\n",
        "    # Labels\n",
        "    real = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    \n",
        "    # Training loop\n",
        "    for iteration in range(iterations):\n",
        "        \n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        ingred = X_train[idx]\n",
        "\n",
        "        z = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        gen_ingred = generator.predict(z)\n",
        "        \n",
        "        d_loss_acc_real = discriminator.train_on_batch(ingred, real)\n",
        "        d_loss_acc_fake = discriminator.train_on_batch(gen_ingred, fake)\n",
        "        d_loss, accuracy = 0.5 * np.add(d_loss_acc_real, d_loss_acc_fake)\n",
        "        \n",
        "        z = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        gen_ingred = generator.predict(z)\n",
        "        g_loss = gan.train_on_batch(z, real)\n",
        "\n",
        "        if iteration % sample_interval == 0:\n",
        "\n",
        "            storeloss.append((d_loss, g_loss))\n",
        "            accuracies.append(100.0 * accuracy)\n",
        "            iteration_checkpoints.append(iteration)\n",
        "\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n",
        "                  (iteration, d_loss, 100.0 * accuracy, g_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMcRIQFK87Na"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwSKogtz9AqD"
      },
      "source": [
        "iterations = 10000\n",
        "batch_size = 128\n",
        "sample_interval = 50\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F296x4a9Cnz"
      },
      "source": [
        "latent_space_vector = np.random.normal(0, 1, (1, latent_dim))\n",
        "arr = generator.predict(latent_space_vector)[0]\n",
        "\n",
        "# count = np.count_nonzero(arr>0.5)\n",
        "# for item in np.nditer(arr):\n",
        "#     if item>0.5:\n",
        "#         print(item)\n",
        "# print('Total occurences of \">0.5\" in array: ', count)\n",
        "\n",
        "index = np.where(arr>0.5)\n",
        "if index==0:\n",
        "    print(\"none\")\n",
        "for num in index[0]:\n",
        "    print(ingredientVectors[1][num])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU6LIF0Y9ErB"
      },
      "source": [
        "latent_space_vector = np.random.normal(0, 1, (1, latent_dim))\n",
        "arr = generator.predict(latent_space_vector)[0]\n",
        "\n",
        "seed = \"INGREDIENTS: \"\n",
        "\n",
        "for index in range(len(arr)):\n",
        "    if arr[index]>0.5:\n",
        "        if len(seed)<15:\n",
        "            seed = seed + ingredientVectors[1][index]\n",
        "        else: \n",
        "            seed = seed + \", \" + ingredientVectors[1][index]\n",
        "        \n",
        "seed = seed + \"\\n\\n1. \"\n",
        "        \n",
        "print(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX4x8hJZN5ld"
      },
      "source": [
        "# **pull data out of csv and format to list of ingredients + directions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMI_4WzjN3mk"
      },
      "source": [
        "import csv\n",
        "import re \n",
        "\n",
        "datafile = open('RecipeNLG_dataset.csv', 'r')\n",
        "datareader = csv.reader(datafile, delimiter=',')\n",
        "originalData = []\n",
        "for row in datareader:\n",
        "    originalData.append(row)    \n",
        "#print (originalData[1])\n",
        "\n",
        "for item in originalData[1]:\n",
        "    print (item + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA5w3IoQNyv5"
      },
      "source": [
        "ingredSteps = []\n",
        "\n",
        "for line in originalData:\n",
        "    new = line[3].split('\", ')\n",
        "    regex = re.compile('[^a-zA-Z| |, | 0-9]')\n",
        "\n",
        "    steps = []\n",
        "\n",
        "    for part in new:\n",
        "        steps.append(regex.sub('', part))\n",
        "    \n",
        "    counter = 1\n",
        "    stepsFinal = \"\"\n",
        "\n",
        "    for part in steps:\n",
        "        temp = str(counter) + \". \" + part\n",
        "        stepsFinal = stepsFinal + temp + \"\\n\"\n",
        "        counter=counter+1\n",
        "    \n",
        "    line2 = line[6]\n",
        "    regex = re.compile('[^a-zA-Z| |,]')\n",
        "    temp = regex.sub('', line2).split(\", \")\n",
        "\n",
        "    tempIngred = \"\"\n",
        "\n",
        "    for index in range(len(temp)):\n",
        "        if index==0:\n",
        "            tempIngred = temp[0]\n",
        "        else: tempIngred += \", \" + temp[index]\n",
        "\n",
        "    final = (\"INGREDIENTS: \" + tempIngred + \"\\n\\n\" + stepsFinal).rstrip(stepsFinal[-1])\n",
        "    ingredSteps.append(final)\n",
        "\n",
        "for line in ingredSteps[1:10]:\n",
        "    print(line)\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM0QNRCxVLt8"
      },
      "source": [
        "CODE for the RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52G44NB-VOZl"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import requests as rq\n",
        "import sys\n",
        "import io\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import keras\n",
        "import keras.callbacks\n",
        "from keras.callbacks import TensorBoard\n",
        "%load_ext tensorboard\n",
        "text = \"\\n\\n\".join(ingredSteps)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Q_YN1qWCBH"
      },
      "source": [
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "seqlen = 80\n",
        "step = 40\n",
        "sentences = []\n",
        "for recipe in ingredSteps[0:200]:\n",
        "    for i in range(0, len(recipe) - seqlen - 1, step):\n",
        "        sentences.append(recipe[i: i + seqlen + 1])\n",
        "\n",
        "\n",
        "x = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), seqlen, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    \n",
        "    for t, (char_in, char_out) in enumerate(zip(sentence[:-1], sentence[1:])):\n",
        "        x[i, t, char_indices[char_in]] = 1\n",
        "        y[i, t, char_indices[char_out]] = 1\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(seqlen, len(chars)), return_sequences=True))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=RMSprop(learning_rate=0.01),\n",
        "    metrics=['categorical_crossentropy', 'accuracy']\n",
        ")\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"Helper function to sample an index from a probability array.\"\"\"\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.exp(np.log(preds) / temperature)  # softmax\n",
        "    preds = preds / np.sum(preds)                #\n",
        "    probas = np.random.multinomial(1, preds, 1)  # sample index\n",
        "    return np.argmax(probas)                     #\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    \"\"\"Function invoked at end of each epoch. Prints generated text.\"\"\"\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - seqlen - 1)\n",
        "    \n",
        "    \n",
        "    for diversity in [0.5, 1.0]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + seqlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, seqlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "            \n",
        "           \n",
        "            preds = model.predict(x_pred, verbose=0)\n",
        "            next_index = sample(preds[0, -1], diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=250,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSFeEH9IWLya"
      },
      "source": [
        "CODE for running the RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVIT6A9tVqtk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji3BxQYLWBF7"
      },
      "source": [
        "for diversity in [0.1,0.5,1]:\n",
        "  # To run with GAN's output change text to seed \n",
        "    sentence = text[8:88]\n",
        "    print(sentence)\n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, seqlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "        \n",
        "        preds = model.predict(x_pred, verbose=0)\n",
        "        next_index = sample(preds[0, -1], diversity)\n",
        "        next_char = indices_char[next_index]\n",
        "        sentence = sentence[1:] + next_char\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "        \n",
        "    print('\\n')\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}